{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.datasets as datasets\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport IPython\nimport PIL.Image\nimport glob\nimport os\nimport cv2\nimport copy\nimport numpy as np\nimport matplotlib.pyplot as plt\n!pip install albumentations==0.5.2\nimport albumentations as albu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-13T17:08:39.103928Z","iopub.execute_input":"2021-08-13T17:08:39.104447Z","iopub.status.idle":"2021-08-13T17:08:50.664238Z","shell.execute_reply.started":"2021-08-13T17:08:39.10434Z","shell.execute_reply":"2021-08-13T17:08:50.663395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install efficientnet_pytorch\nfrom efficientnet_pytorch import EfficientNet\n\ndevice = torch.device('cuda')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T17:10:24.056713Z","iopub.execute_input":"2021-08-13T17:10:24.057076Z","iopub.status.idle":"2021-08-13T17:10:33.146043Z","shell.execute_reply.started":"2021-08-13T17:10:24.057044Z","shell.execute_reply":"2021-08-13T17:10:33.145077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_stop_path='../input/sensor-stop/dataset/dataset'\nmodel_stop_path='../input/sensor-stop/'\ndata_xy_path='../input/dataset-xy/dataset_xy/dataset_xy'\nfocus_xy_path='../input/dataset-xy/test_focus/test_focus/'\nmodel_xy_path='../input/dataset-xy/'\nmulti_path='../input/multi-model/'","metadata":{"execution":{"iopub.status.busy":"2021-08-13T17:10:33.149493Z","iopub.execute_input":"2021-08-13T17:10:33.14981Z","iopub.status.idle":"2021-08-13T17:10:33.158132Z","shell.execute_reply.started":"2021-08-13T17:10:33.149777Z","shell.execute_reply":"2021-08-13T17:10:33.157349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### For efficiency the classifier should reuse the backbone of the regression model","metadata":{}},{"cell_type":"markdown","source":"#### Loading a backbone pretained on the regression task","metadata":{}},{"cell_type":"code","source":"# Load road following model\nmodel = EfficientNet.from_name('efficientnet-b0')\nmodel._fc=torch.nn.Linear(model._fc.in_features, 2)\nmodel.load_state_dict(torch.load(model_xy_path+'efficientnet_b0_steering_model_xy_tape.pth'))\n# Copy final layer\nmodel_xy_top=torch.nn.Linear(model._fc.in_features, 2)\nmodel_xy_top.load_state_dict(model._fc.state_dict())\nmodel_xy_top=model_xy_top.to(device)\n# Turn the model into a backbone/feature extractor\nbackbone = model.to(device)\nbackbone._fc = torch.nn.Identity()\nbackbone.eval()\nprint(\"\")","metadata":{"execution":{"iopub.status.busy":"2021-08-04T23:02:52.720762Z","iopub.execute_input":"2021-08-04T23:02:52.721118Z","iopub.status.idle":"2021-08-04T23:02:53.35409Z","shell.execute_reply.started":"2021-08-04T23:02:52.721088Z","shell.execute_reply":"2021-08-04T23:02:53.353106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Loading the backbone only pretrained on ImageNet","metadata":{}},{"cell_type":"code","source":"# Load ImageNet model\nbackbone = EfficientNet.from_pretrained('efficientnet-b0')\n# Create regression head\nregressor=torch.nn.Linear(backbone._fc.in_features, 2)\nregressor.to(device)\n# Turn the model into a backbone/feature extractor\nbackbone._fc = torch.nn.Identity()\nbackbone = backbone.to(device)\nprint(\"\")","metadata":{"execution":{"iopub.status.busy":"2021-08-04T23:03:36.107999Z","iopub.execute_input":"2021-08-04T23:03:36.108416Z","iopub.status.idle":"2021-08-04T23:03:41.580289Z","shell.execute_reply.started":"2021-08-04T23:03:36.108366Z","shell.execute_reply":"2021-08-04T23:03:41.578779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initialize the classifier","metadata":{}},{"cell_type":"code","source":"# model0: 64, no dropout\n\nclassifier = torch.nn.Sequential(\n    torch.nn.Linear(model_xy_top.in_features, 256),\n    torch.nn.SiLU(),\n    torch.nn.Dropout(p=0.2, inplace=False),\n    torch.nn.Linear(256, 2),\n)\n\nclassifier=classifier.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T18:18:04.707276Z","iopub.execute_input":"2021-08-04T18:18:04.707829Z","iopub.status.idle":"2021-08-04T18:18:04.725529Z","shell.execute_reply.started":"2021-08-04T18:18:04.70779Z","shell.execute_reply":"2021-08-04T18:18:04.724563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier = torch.nn.Linear(regressor.in_features, 2)\nclassifier=classifier.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T23:03:41.58148Z","iopub.execute_input":"2021-08-04T23:03:41.581789Z","iopub.status.idle":"2021-08-04T23:03:41.586957Z","shell.execute_reply.started":"2021-08-04T23:03:41.581762Z","shell.execute_reply":"2021-08-04T23:03:41.585741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create the multi-headed network","metadata":{}},{"cell_type":"code","source":"class MultiHeadNetwork(torch.nn.Module):\n    '''\n    This class only serves for training purposes, \n    the components get saved and deployed separately.'''\n    def __init__(self,backbone,head_xy,head_cl):\n        super(MultiHeadNetwork,self).__init__()\n\n        # This represents the shared layer(s) before the different heads\n        # Here, I used a single linear layer for simplicity purposes\n        # But any network configuration should work\n        self.shared = backbone\n\n        # Set up the different heads\n        # Each head can take any network configuration\n        self.head_xy = head_xy\n        self.head_cl = head_cl\n\n    def forward(self, x, head):\n\n        # Run the shared layer(s)\n        #x = self.shared(x)\n        # Run the different heads with the output of the shared layers as input\n        if head==\"xy\":\n            y = self.head_xy(self.shared(x))\n        elif head==\"cl\":\n            y = self.head_cl(self.shared(x))\n\n        return y\n\n\n#multi_model = MultiHeadNetwork(backbone,regressor,classifier)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T17:11:24.569731Z","iopub.execute_input":"2021-08-13T17:11:24.57005Z","iopub.status.idle":"2021-08-13T17:11:24.57606Z","shell.execute_reply.started":"2021-08-13T17:11:24.570021Z","shell.execute_reply":"2021-08-13T17:11:24.575259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def motion_blur():\n    train_transform = [\n        albu.MotionBlur(blur_limit=30, p=1)        \n    ]\n    return albu.Compose(train_transform)\n\nmotion_blur_composed = motion_blur()\n\nclass CustomImageFolder(datasets.ImageFolder):\n  def __getitem__(self, index: int):\n    path, target = self.samples[index]\n    sample = self.loader(path)\n    if float(np.random.rand(1)) > 0.1:\n      sample = np.array(sample)\n      sample = motion_blur_composed(image=sample)\n      sample = PIL.Image.fromarray(sample['image'])\n    if self.transform is not None:\n        sample = self.transform(sample)\n    if self.target_transform is not None:\n        target = self.target_transform(target)\n\n    return sample, target\n\n\ndef motion_blur():\n    train_transform = [\n        albu.MotionBlur(blur_limit=30, p=1)        \n    ]\n    return albu.Compose(train_transform)\n\ndef get_x(path, width):\n    \"\"\"Gets the x value from the image filename\"\"\"\n    return (float(int(path.split(\"_\")[1])) - width/2) / (width/2)\n\ndef get_y(path, height):\n    \"\"\"Gets the y value from the image filename\"\"\"\n    return (float(int(path.split(\"_\")[2])) - height/2) / (height/2)\n\nclass XYDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, directory, random_hflips=False, col_params=(0.3,0,0,0), p_blur=0.1):\n        self.directory = directory\n        self.random_hflips = random_hflips\n        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))\n        self.color_jitter = transforms.ColorJitter(brightness=col_params[0],\n                                                   contrast=col_params[1],\n                                                   saturation=col_params[2],\n                                                   hue=col_params[3])\n        self.motion_blur = motion_blur()\n        self.p_blur=p_blur\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        \n        image = PIL.Image.open(image_path)\n        width, height = image.size\n        x = float(get_x(os.path.basename(image_path), width))\n        y = float(get_y(os.path.basename(image_path), height))\n      \n        if float(np.random.rand(1)) > 0.5:\n            image = transforms.functional.hflip(image)\n            x = -x\n        \n        if float(np.random.rand(1)) < self.p_blur:\n            image_np = np.array(image)\n            image_np = self.motion_blur(image=image_np)\n            image = PIL.Image.fromarray(image_np['image'])\n\n        image = self.color_jitter(image)\n        image = transforms.functional.resize(image, (224, 224))\n        image = transforms.functional.to_tensor(image)\n        image = image.numpy()[::-1].copy()\n        image = torch.from_numpy(image)\n        # ImageNet mean and stdev\n        image = transforms.functional.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        \n        return image, torch.tensor([x, y]).float()\n\nclass InspectionDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, directory, random_hflips=False, col_params=(0.3,0,0,0), p_blur=0.1):\n        self.directory = directory\n        self.random_hflips = random_hflips\n        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))\n        self.color_jitter = transforms.ColorJitter(brightness=col_params[0],\n                                                   contrast=col_params[1],\n                                                   saturation=col_params[2],\n                                                   hue=col_params[3])\n        self.motion_blur = motion_blur()\n        self.p_blur=p_blur\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        \n        image = PIL.Image.open(image_path)\n        width, height = image.size\n        x = float(get_x(os.path.basename(image_path), width))\n        y = float(get_y(os.path.basename(image_path), height))\n      \n        if float(np.random.rand(1)) > 0.5:\n            image = transforms.functional.hflip(image)\n            x = -x\n        \n        if float(np.random.rand(1)) < self.p_blur:\n            image_np = np.array(image)\n            image_np = self.motion_blur(image=image_np)\n            image = PIL.Image.fromarray(image_np['image'])\n\n        image = self.color_jitter(image)\n        image = transforms.functional.resize(image, (224, 224))\n        image = transforms.functional.to_tensor(image)\n        image_norm = image.numpy()[::-1].copy()\n        image_norm = torch.from_numpy(image_norm)\n        # ImageNet mean and stdev\n        image_norm = transforms.functional.normalize(image_norm, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        \n        return image_norm, torch.tensor([x, y]).float(), image\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T17:11:24.884659Z","iopub.execute_input":"2021-08-13T17:11:24.884963Z","iopub.status.idle":"2021-08-13T17:11:24.910824Z","shell.execute_reply.started":"2021-08-13T17:11:24.884933Z","shell.execute_reply":"2021-08-13T17:11:24.909936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_xy = XYDataset(data_xy_path, random_hflips=True)\n\ndataset_stop = CustomImageFolder(\n    data_stop_path,\n    transforms.Compose([\n        transforms.ColorJitter(brightness=0.3, contrast=0, saturation=0, hue=0),\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n)\ndataset_stop.class_to_idx ","metadata":{"execution":{"iopub.status.busy":"2021-08-13T17:11:25.780256Z","iopub.execute_input":"2021-08-13T17:11:25.780612Z","iopub.status.idle":"2021-08-13T17:11:25.803988Z","shell.execute_reply.started":"2021-08-13T17:11:25.780582Z","shell.execute_reply":"2021-08-13T17:11:25.803003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_percent = 0.2\nnum_test = int(test_percent * len(dataset_stop))\ntorch.manual_seed(2021)\ntrain_stop_dataset, test_stop_dataset = torch.utils.data.random_split(dataset_stop, [len(dataset_stop) - num_test, num_test])\nnum_test = int(test_percent * len(dataset_xy))\ntorch.manual_seed(2021)\ntrain_xy_dataset, test_xy_dataset = torch.utils.data.random_split(dataset_xy, [len(dataset_xy) - num_test, num_test])\n# Add hand picked test images to make sure it fixes errors\ntest_extra_xy_dataset=torch.utils.data.ConcatDataset([test_xy_dataset,XYDataset(focus_xy_path, random_hflips=True)])","metadata":{"execution":{"iopub.status.busy":"2021-08-13T17:11:26.661939Z","iopub.execute_input":"2021-08-13T17:11:26.662254Z","iopub.status.idle":"2021-08-13T17:11:26.671588Z","shell.execute_reply.started":"2021-08-13T17:11:26.662223Z","shell.execute_reply":"2021-08-13T17:11:26.670755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_stop_loader = torch.utils.data.DataLoader(\n    train_stop_dataset,\n    batch_size=16,\n    shuffle=True,\n    num_workers=0\n)\n\ntest_stop_loader = torch.utils.data.DataLoader(\n    test_stop_dataset,\n    batch_size=16,\n    shuffle=True,\n    num_workers=0\n)\n\ntrain_xy_loader = torch.utils.data.DataLoader(\n    train_xy_dataset,\n    batch_size=16,\n    shuffle=True,\n    num_workers=0\n)\n\ntest_xy_loader = torch.utils.data.DataLoader(\n    test_extra_xy_dataset,\n    batch_size=16,\n    shuffle=True,\n    num_workers=0\n)\n\ninspection_loader = torch.utils.data.DataLoader(\n    InspectionDataset(focus_xy_path, random_hflips=True,col_params=(0.3,0,0,0),p_blur=1),\n    batch_size=16,\n    shuffle=False,\n    num_workers=0\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T17:11:27.283425Z","iopub.execute_input":"2021-08-13T17:11:27.283753Z","iopub.status.idle":"2021-08-13T17:11:27.291607Z","shell.execute_reply.started":"2021-08-13T17:11:27.283724Z","shell.execute_reply":"2021-08-13T17:11:27.290535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training regression and classifier heads together","metadata":{}},{"cell_type":"code","source":"backbone = EfficientNet.from_name('efficientnet-b0')\nregressor = torch.nn.Linear(1280, 2)\nclassifier = torch.nn.Linear(1280, 2)\nbackbone._fc = torch.nn.Identity()\n\nbackbone = backbone.to(device)\nregressor = regressor.to(device)\nclassifier = classifier.to(device)\n\nbackbone.load_state_dict(torch.load(multi_path+'backbone_0.pth'))\nregressor.load_state_dict(torch.load(multi_path+'head_xy_0.pth'))\nclassifier.load_state_dict(torch.load(multi_path+'head_stop_0.pth'))\nmulti_model = MultiHeadNetwork(backbone,regressor,classifier)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T17:11:30.304701Z","iopub.execute_input":"2021-08-13T17:11:30.305036Z","iopub.status.idle":"2021-08-13T17:11:30.481024Z","shell.execute_reply.started":"2021-08-13T17:11:30.305006Z","shell.execute_reply":"2021-08-13T17:11:30.480232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_EPOCHS = 300\nBEST_SHARED_PATH = 'backbone_1.pth'\nBEST_XY_PATH = 'head_xy_1.pth'\nBEST_STOP_PATH = 'head_stop_1.pth'\nbest_loss = 0.191133\n\noptimizer = optim.Adam(multi_model.parameters(),lr=1e-4)\nxy_weight=20.0\nout = display(IPython.display.Pretty('Start training'), display_id=True)\nfor epoch in range(NUM_EPOCHS):\n    multi_model.train()\n    \n    # Train for regression\n    for images, labels in iter(train_xy_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        outputs = multi_model(images,\"xy\")\n        loss_xy = F.mse_loss(outputs, labels)\n        loss_xy.backward()\n        optimizer.step()\n    \n    # Train for classification\n    for images, labels in iter(train_stop_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        outputs = multi_model(images,\"cl\")\n        loss_stop = F.cross_entropy(outputs, labels)\n        loss_stop.backward()\n        optimizer.step()\n        \n    # Train for regression\n    for images, labels in iter(train_xy_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        outputs = multi_model(images,\"xy\")\n        loss_xy = F.mse_loss(outputs, labels)\n        loss_xy.backward()\n        optimizer.step()\n    \n     \n    multi_model.eval()\n    test_xy_loss = 0.0\n    test_stop_loss = 0.0\n    test_multi_loss = 0.0\n    # Evaluate on regression\n    for images, labels in iter(test_xy_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = multi_model(images,\"xy\")\n        loss_xy = F.mse_loss(outputs, labels)\n        test_xy_loss += float(loss_xy)\n    test_xy_loss /= len(test_xy_loader)\n    \n    # Evaluate on classification\n    for images, labels in iter(test_stop_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = multi_model(images,\"cl\")\n        loss_stop = F.cross_entropy(outputs, labels)\n        test_stop_loss += float(loss_stop)\n    test_stop_loss /= len(test_stop_loader)\n    \n    test_multi_loss=xy_weight*test_xy_loss+test_stop_loss\n    \n    out.update(IPython.display.Pretty('Epoch %d | xy: %f, stop: %f, joint: %f' % (epoch,test_xy_loss, test_stop_loss, test_multi_loss)))\n\n    if test_multi_loss < best_loss:\n        torch.save(multi_model.shared.state_dict(), BEST_SHARED_PATH)\n        torch.save(multi_model.head_xy.state_dict(), BEST_XY_PATH)\n        torch.save(multi_model.head_cl.state_dict(), BEST_STOP_PATH)\n        best_loss = test_multi_loss\n        print('Epoch %d | xy: %f, stop: %f, joint: %f' % (epoch,test_xy_loss, test_stop_loss, test_multi_loss))","metadata":{"execution":{"iopub.status.busy":"2021-08-06T14:28:02.126352Z","iopub.execute_input":"2021-08-06T14:28:02.126728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training classifier head on regression backbone","metadata":{}},{"cell_type":"code","source":"#classifier.load_state_dict(torch.load(model_path+'stop_model.pth'))\n#classifier.load_state_dict(torch.load('./'+'stop_model_3.pth'))","metadata":{"execution":{"iopub.status.busy":"2021-08-04T20:38:15.468585Z","iopub.execute_input":"2021-08-04T20:38:15.468942Z","iopub.status.idle":"2021-08-04T20:38:15.478465Z","shell.execute_reply.started":"2021-08-04T20:38:15.468912Z","shell.execute_reply":"2021-08-04T20:38:15.477491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_EPOCHS = 300\nBEST_MODEL_PATH = 'stop_model_3.pth'\nbest_loss = 0.131893\n# 0.069680\n# 0.123112\n# 0.133830\n# 0.131893\n\noptimizer = optim.Adam(classifier.parameters(),lr=1e-5)\nout = display(IPython.display.Pretty('Start training'), display_id=True)\nfor epoch in range(NUM_EPOCHS):\n    \n    classifier.train()\n    for images, labels in iter(train_stop_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        outputs = classifier(backbone(images))\n        loss = F.cross_entropy(outputs, labels)\n        loss.backward()\n        optimizer.step()\n    \n    classifier.eval()\n    test_loss = 0.0\n    for images, labels in iter(test_stop_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = classifier(backbone(images))\n        loss = F.cross_entropy(outputs, labels)\n        test_loss += float(loss)\n    test_loss /= len(test_loader)\n    out.update(IPython.display.Pretty('%d: %f' % (epoch, test_loss)))\n\n    if test_loss < best_loss:\n        torch.save(classifier.state_dict(), BEST_MODEL_PATH)\n        best_loss = test_loss\n        print('%d: %f' % (epoch, test_loss))","metadata":{"execution":{"iopub.status.busy":"2021-08-04T20:38:19.108553Z","iopub.execute_input":"2021-08-04T20:38:19.108921Z","iopub.status.idle":"2021-08-04T21:05:55.155448Z","shell.execute_reply.started":"2021-08-04T20:38:19.108891Z","shell.execute_reply":"2021-08-04T21:05:55.153248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reuse backbone for road following!","metadata":{}},{"cell_type":"markdown","source":"### Testing","metadata":{}},{"cell_type":"code","source":"def scale_y(y):\n    return (0.5 - y) / 2.0\ndef scale_to_pixels(x, width=224):\n    return int(x*(224/2)+(224/2))\n#model = EfficientNet.from_name('efficientnet-b0')\n#model._fc=torch.nn.Linear(model_road._fc.in_features, 2)\n#model.load_state_dict(torch.load(model_path+'efficientnet_b0_steering_model_xy_tape.pth'))\nmulti_model.eval()\nfor images_norm, labels, images in iter(inspection_loader):\n    images_norm = images_norm.to(device)\n    outputs = multi_model(images_norm,\"xy\")\n    xy_batch=outputs.detach().float().cpu().numpy()\n    #for i in range(images.shape[0]):\n    for i in range(12):\n        x_hat=scale_to_pixels(xy_batch[i,0])\n        y_hat=scale_to_pixels(xy_batch[i,1])\n        x=scale_to_pixels(labels.detach().float().cpu().numpy()[i][0])\n        y=scale_to_pixels(labels.detach().float().cpu().numpy()[i][1])\n        # convert tensors back to images to add circles\n        image_np=images[i].permute(1, 2, 0).numpy()* 255\n        image_np=image_np.astype(np.uint8)\n        overlay1=cv2.circle(image_np.copy(), (x, y), 8, (0, 255, 0), 3)\n        overlay2=cv2.circle(image_np.copy(), (x_hat, y_hat), 8, (255, 0, 0), 3)\n        alpha=0.5\n        image_np=cv2.addWeighted(overlay2, alpha, overlay1, 1 - alpha, 0)\n        # and then back to tensors for plotting\n        images[i]=transforms.functional.to_tensor(image_np)\n    grid_img = torchvision.utils.make_grid(images[0:12,:,:], nrow=4)\n    fig, ax = plt.subplots(figsize=(18, 10))\n    ax.imshow(grid_img.permute(1, 2, 0))\n    plt.tight_layout()\n    fig.savefig(\"regression_example.pdf\")","metadata":{"execution":{"iopub.status.busy":"2021-08-13T17:16:15.303803Z","iopub.execute_input":"2021-08-13T17:16:15.304123Z","iopub.status.idle":"2021-08-13T17:16:16.467442Z","shell.execute_reply.started":"2021-08-13T17:16:15.304093Z","shell.execute_reply":"2021-08-13T17:16:16.466647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}